{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-24T23:15:50.515187800Z",
     "start_time": "2023-12-24T23:15:50.491346500Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as sk\n",
    "import skimage as ski\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Data Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cifar = tf.keras.datasets.cifar10.load_data()\n",
    "(x_train, y_train), (x_test, y_test) = cifar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-24T23:38:33.482589600Z",
     "start_time": "2023-12-24T23:38:22.893889300Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 1000 into shape (32,32,3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 67\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;66;03m# \u001b[39;00m\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;66;03m# new_x_train = np.empty(x_train.shape)\u001b[39;00m\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;66;03m# new_y_train = None\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;66;03m# x_test = new_x_test\u001b[39;00m\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;66;03m# y_test = new_y_test\u001b[39;00m\n\u001b[0;32m     66\u001b[0m n_samples_from_each_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m---> 67\u001b[0m \u001b[43mget_N_from_each_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_samples_from_each_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(x_train))\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28mprint\u001b[39m(x_train)\n",
      "Cell \u001b[1;32mIn[47], line 13\u001b[0m, in \u001b[0;36mget_N_from_each_class\u001b[1;34m(N, percentage_of_testing_data, x_train, y_train, x_test, y_test)\u001b[0m\n\u001b[0;32m     11\u001b[0m y_train \u001b[38;5;241m=\u001b[39m y_train[selected_indices]\n\u001b[0;32m     12\u001b[0m x_train \u001b[38;5;241m=\u001b[39m x_train\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m---> 13\u001b[0m y_train \u001b[38;5;241m=\u001b[39m \u001b[43my_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m selected_indices \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m class_label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 1000 into shape (32,32,3)"
     ]
    }
   ],
   "source": [
    "# Taking a number of data points from each class\n",
    "def get_N_from_each_class(N, percentage_of_testing_data, x_train, y_train, x_test, y_test):\n",
    "    # TODO\n",
    "    \n",
    "    selected_indices = []\n",
    "    for class_label in range(10):\n",
    "        class_indices = [i for i in range(len(y_train)) if y_train[i][0] == class_label]\n",
    "        selected_indices.append(class_indices[:N])\n",
    "        \n",
    "    x_train = x_train[selected_indices]\n",
    "    y_train = y_train[selected_indices]\n",
    "    x_train = x_train.reshape(-1, 32, 32, 3)\n",
    "    y_train = y_train.reshape(-1, 32, 32, 3)\n",
    "    \n",
    "    \n",
    "    selected_indices = []\n",
    "    for class_label in range(10):\n",
    "        class_indices = [i for i in range(len(y_test)) if y_test[i][0] == class_label]\n",
    "        selected_indices.append(class_indices[:N*percentage_of_testing_data])\n",
    "        \n",
    "    x_test = x_test[selected_indices]\n",
    "    y_test = y_test[selected_indices]\n",
    "    x_test = x_test.reshape(-1, 32, 32, 3)\n",
    "    y_test = y_test.reshape(-1, 32, 32, 3)\n",
    "    \n",
    "    \n",
    "    # \n",
    "    # new_x_train = np.empty(x_train.shape)\n",
    "    # new_y_train = None\n",
    "    # \n",
    "    # new_x_test = None\n",
    "    # new_y_test = None\n",
    "    # \n",
    "    # for class_index in range(10):\n",
    "    #     # Getting all data from that class from x_train\n",
    "    #     # class_x_train = [i for i in x_train if y_train[i] == class_index]\n",
    "    #     class_x_train = np.where()\n",
    "    #     \n",
    "    #     # Picking only N random datapoint from them\n",
    "    #     class_x_train = class_x_train[:N]\n",
    "    #     \n",
    "    #     class_y_train = y_train[y_train.flatten() == class_index]\n",
    "    #     class_y_train = class_y_train[:N]\n",
    "    #     \n",
    "    #     # Adding the data to the new data\n",
    "    #     new_x_train += class_x_train\n",
    "    #     new_y_train += class_y_train\n",
    "    #     \n",
    "    # # Doing the same thing with testing data as the previous for loop\n",
    "    # for class_index in range(10):\n",
    "    #     class_x_test = [i for i in x_test if y_test[i] == class_index]\n",
    "    #     class_x_test = class_x_test[np.random.choice(len(class_x_test), size=N*percentage_of_testing_data, replace=False)]\n",
    "    #     \n",
    "    #     class_y_test = [i for i in y_test if y_test[i] == class_index]\n",
    "    #     class_y_test = class_y_test[np.random.choice(len(class_y_test), size=N*percentage_of_testing_data, replace=False)]\n",
    "    #     \n",
    "    #     new_x_test += class_x_test\n",
    "    #     new_y_test += class_y_test\n",
    "    # \n",
    "    # # Replacing original data with new data\n",
    "    # x_train = new_x_train\n",
    "    # y_train = new_y_train\n",
    "    # x_test = new_x_test\n",
    "    # y_test = new_y_test\n",
    "\n",
    "n_samples_from_each_class = 100\n",
    "get_N_from_each_class(n_samples_from_each_class, 0.7, x_train, y_train, x_test, y_test)\n",
    "print(len(x_train))\n",
    "print(x_train)\n",
    "\n",
    "# print(y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extrating the features from the images\n",
    "# method used: Central Moments\n",
    "\n",
    "def extract_image_features(image , C = 3):\n",
    "    '''\n",
    "    Extracts the features from the image using the central moments method of order C\n",
    "    It turns the image to grayscale, then calculates the central moments of order C\n",
    "    \n",
    "    Parameters:\n",
    "    image: the image to be processed\n",
    "    C: the moments order\n",
    "\n",
    "    Returns:\n",
    "    features: a vector of the features extracted from the image\n",
    "    '''\n",
    "\n",
    "    # turning the image to grayscale\n",
    "    image = ski.color.rgb2gray(image)\n",
    "\n",
    "    # calculating the central moments\n",
    "    cal_moments = ski.measure.moments_central(image, order = C)\n",
    "\n",
    "    # create an np array that contains zeroes \n",
    "    features = np.zeros(shape = int((C + 1) * ( C + 2) / 2))\n",
    "\n",
    "\n",
    "    # turn the 2D array to 1D by putting only the needed values based on this equation \n",
    "    # p + q <= C \n",
    "    # if C = 3 the needed numbers are : 00 01 10 11 20 02 21 12 03 30 , size = 10 s\n",
    "    current = 0;\n",
    "    for i in range (C + 1):\n",
    "        for j in range (C + 1):\n",
    "            if(i + j <= C):\n",
    "                features[current] = cal_moments[i][j]\n",
    "                current = current + 1\n",
    "\n",
    "                \n",
    "    return features    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display a sample of images\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 6))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(x_train[i])\n",
    "    ax.axis('off')\n",
    "    ax.set_title(f'Label: {y_train[i][0]}')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SVM \n",
    "<b>(An Example for Multi-Class Classification using SVM)</b>\n",
    "\n",
    "https://jakevdp.github.io/PythonDataScienceHandbook/05.07-support-vector-machines.html\n",
    "\n",
    "- Here is what we going to do for the SVM:\n",
    "    * We will use the SVM implementation in sklearn\n",
    "    * The parameters of the SVM will be chosen using grid search with cross validation\n",
    "        * Degree for the polynomial kernel\n",
    "        * C parameter for the SVM\n",
    "            * C (Regularization Parameter): The `C` parameter is a positive scalar that controls the regularization strength. It influences the optimization process by balancing the desire to fit the training data well against the goal of keeping the model simple and avoiding overfitting.\n",
    "            * Smaller values of C result in a wider margin, allowing more training points to be misclassified.\n",
    "            * Larger values of C result in a narrower margin, forcing the optimizer to classify all training points correctly.\n",
    "        * Gamma parameter for `rbf` , `poly` and `sigmoid` kernels\n",
    "        * we will use both OVR and OVO strategies (`decision_function_shape` parameter)\n",
    "    \n",
    "    * We will Grid Search to find the best parameters for the SVM\n",
    "    * We will use the `make_pipeline` \n",
    "    * We will use the `StandardScaler` to scale the data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### KNN \n",
    "\n",
    "( An Example for KNN Classification )\n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/neighbors/plot_classification.html#sphx-glr-auto-examples-neighbors-plot-classification-py\n",
    "\n",
    "* The parameter we are going to experiment with the value of K (number of neighbors)\n",
    "* The same thing as in SVM we are going to use `make_pipeline` and `StandardScaler` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
