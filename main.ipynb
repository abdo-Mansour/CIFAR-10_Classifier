{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-24T23:15:50.515187800Z",
     "start_time": "2023-12-24T23:15:50.491346500Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as sk\n",
    "import skimage as ski\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Data Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-25T00:27:12.333561800Z",
     "start_time": "2023-12-25T00:27:11.620423600Z"
    }
   },
   "outputs": [],
   "source": [
    "cifar = tf.keras.datasets.cifar10.load_data()\n",
    "(x_train, y_train), (x_test, y_test) = cifar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-25T00:29:00.063000900Z",
     "start_time": "2023-12-25T00:29:00.001998800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 32, 32, 3)\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "# Taking a number of data points from each class\n",
    "def get_N_from_each_class(N, percentage_of_testing_data, x_train, y_train, x_test, y_test):\n",
    "    # Generating indices for the data points we are going to take from the data (both test and train)\n",
    "    selected_indices = []\n",
    "    for class_label in range(10):\n",
    "        # For every possible index, take the index iff the point belongs to this class \n",
    "        class_indices = [i for i in range(len(y_train)) if y_train[i][0] == class_label]\n",
    "        selected_indices.append(class_indices[:N])\n",
    "    \n",
    "    # Picking the data based on the indices\n",
    "    x_train = x_train[selected_indices]\n",
    "    y_train = y_train[selected_indices]\n",
    "    # Reshaping the data to flatten it to all classes together (to get it back to original form)\n",
    "    x_train = x_train.reshape(-1, 32, 32, 3)\n",
    "    y_train = y_train.reshape(-1, 1)\n",
    "    \n",
    "    \n",
    "    # Doing the same with the test data\n",
    "    selected_indices = []\n",
    "    for class_label in range(10):\n",
    "        class_indices = [i for i in range(len(y_test)) if y_test[i][0] == class_label]\n",
    "        selected_indices.append(class_indices[:int(N*percentage_of_testing_data)])\n",
    "        \n",
    "    x_test = x_test[selected_indices]\n",
    "    y_test = y_test[selected_indices]\n",
    "    x_test = x_test.reshape(-1, 32, 32, 3)\n",
    "    y_test = y_test.reshape(-1, 1)\n",
    "    \n",
    "    # Randomizing data, just in case\n",
    "    np.random.shuffle(x_train)\n",
    "    np.random.shuffle(y_train)\n",
    "    np.random.shuffle(x_test)\n",
    "    np.random.shuffle(y_test)\n",
    "    \n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "\n",
    "n_samples_from_each_class = 100\n",
    "(x_train, y_train), (x_test, y_test) = get_N_from_each_class(n_samples_from_each_class, 0.7, x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extrating the features from the images\n",
    "# method used: Central Moments\n",
    "\n",
    "def extract_image_features(image , C = 3):\n",
    "    '''\n",
    "    Extracts the features from the image using the central moments method of order C\n",
    "    It turns the image to grayscale, then calculates the central moments of order C\n",
    "    \n",
    "    Parameters:\n",
    "    image: the image to be processed\n",
    "    C: the moments order\n",
    "\n",
    "    Returns:\n",
    "    features: a vector of the features extracted from the image\n",
    "    '''\n",
    "\n",
    "    # turning the image to grayscale\n",
    "    image = ski.color.rgb2gray(image)\n",
    "\n",
    "    # calculating the central moments\n",
    "    cal_moments = ski.measure.moments_central(image, order = C)\n",
    "\n",
    "    # create an np array that contains zeroes \n",
    "    features = np.zeros(shape = int((C + 1) * ( C + 2) / 2))\n",
    "\n",
    "\n",
    "    # turn the 2D array to 1D by putting only the needed values based on this equation \n",
    "    # p + q <= C \n",
    "    # if C = 3 the needed numbers are : 00 01 10 11 20 02 21 12 03 30 , size = 10 s\n",
    "    current = 0;\n",
    "    for i in range (C + 1):\n",
    "        for j in range (C + 1):\n",
    "            if(i + j <= C):\n",
    "                features[current] = cal_moments[i][j]\n",
    "                current = current + 1\n",
    "\n",
    "                \n",
    "    return features    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display a sample of images\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 6))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(x_train[i])\n",
    "    ax.axis('off')\n",
    "    ax.set_title(f'Label: {y_train[i][0]}')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SVM \n",
    "<b>(An Example for Multi-Class Classification using SVM)</b>\n",
    "\n",
    "https://jakevdp.github.io/PythonDataScienceHandbook/05.07-support-vector-machines.html\n",
    "\n",
    "- Here is what we going to do for the SVM:\n",
    "    * We will use the SVM implementation in sklearn\n",
    "    * The parameters of the SVM will be chosen using grid search with cross validation\n",
    "        * Degree for the polynomial kernel\n",
    "        * C parameter for the SVM\n",
    "            * C (Regularization Parameter): The `C` parameter is a positive scalar that controls the regularization strength. It influences the optimization process by balancing the desire to fit the training data well against the goal of keeping the model simple and avoiding overfitting.\n",
    "            * Smaller values of C result in a wider margin, allowing more training points to be misclassified.\n",
    "            * Larger values of C result in a narrower margin, forcing the optimizer to classify all training points correctly.\n",
    "        * Gamma parameter for `rbf` , `poly` and `sigmoid` kernels\n",
    "        * we will use both OVR and OVO strategies (`decision_function_shape` parameter)\n",
    "    \n",
    "    * We will Grid Search to find the best parameters for the SVM\n",
    "    * We will use the `make_pipeline` \n",
    "    * We will use the `StandardScaler` to scale the data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### KNN \n",
    "\n",
    "( An Example for KNN Classification )\n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/neighbors/plot_classification.html#sphx-glr-auto-examples-neighbors-plot-classification-py\n",
    "\n",
    "* The parameter we are going to experiment with the value of K (number of neighbors)\n",
    "* The same thing as in SVM we are going to use `make_pipeline` and `StandardScaler` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
